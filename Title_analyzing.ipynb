{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "PN8dRGwbmDaU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#pd.set_option('display.max_rows', 50000)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PwAWjafmGkY",
    "outputId": "fc2b596b-166a-455d-f1a8-85e51b41c861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:\n",
    "# other library for languages detection ? qualifiers included if exists? \n",
    "# duplicated titles: red queen 1, The Secret Garden\n",
    "# variable ngram\n",
    "# clean the titles --> DONE\n",
    "# try out with other languages\n",
    "# recommend for all the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Udl7QzRmG2x",
    "outputId": "5e2072f8-6cd9-4298-a3ab-2dd3d5461580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78030, 6)\n",
      "(365143, 5)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "#pfad = \"drive/MyDrive/DataMiningSeminar/\"\n",
    "items = pd.read_csv(\"data/items.csv\", delimiter='|')\n",
    "transactions = pd.read_csv(\"data/transactions.csv\", delimiter='|')\n",
    "evaluation = pd.read_csv(\"data/evaluation.csv\", delimiter='|')\n",
    "print(items.shape)\n",
    "print(transactions.shape)\n",
    "print(evaluation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_new = pd.read_csv('items_language.csv')\n",
    "titles_eng = items_new.loc[items_new.language == \"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "def lower_case(title):\n",
    "    return np.char.lower(title)\n",
    "#remove the stop words\n",
    "\n",
    "def remove_stop_words(title, lang):\n",
    "    stop_words = set(stopwords.words(lang))\n",
    "    word_tokens = word_tokenize(title) \n",
    "  \n",
    "    filtered_title = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_title.append(w)\n",
    "    return filtered_title\n",
    "# remove punctuation:\n",
    "def remove_punctuation(title):\n",
    "    symbols = \"!\\\"#$%&()*+-./:';<=>?@[\\]^_`{|}~\\n\"\n",
    "    filtered_title = [] \n",
    "  \n",
    "    for t in title: \n",
    "        if t not in symbols: \n",
    "            filtered_title.append(str(t))\n",
    "    return filtered_title\n",
    "# remove single characters\n",
    "#def remove_single_characters(text):\n",
    "#    new_text = \"\"\n",
    "#    for t in text:\n",
    "#        if len(t) > 1:\n",
    "#            new_text = new_text + \" \" + t\n",
    "#    return new_text\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#def stemming_text(title):\n",
    "    \n",
    "#    filtered_title = []\n",
    "#    for t in title:\n",
    "#        filtered_title.append(stemmer.stem(t))\n",
    "#    return filtered_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thuylinhnguyen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/thuylinhnguyen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/thuylinhnguyen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/thuylinhnguyen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "titles_eng[\"title_processed\"] = titles_eng.title.apply(lambda x: lower_case(str(x)))\n",
    "titles_eng[\"title_processed\"] = titles_eng.title_processed.apply(lambda x: remove_stop_words(str(x), \"english\"))\n",
    "titles_eng[\"title_processed\"] = titles_eng.title_processed.apply(lambda x: remove_punctuation(x))\n",
    "#titles_eng[\"title_processed\"] = titles_eng.title_processed.apply(lambda x: remove_single_characters(str(x)))\n",
    "#titles_eng[\"title_processed\"] = titles_eng.title_processed.apply(lambda x: stemming_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = titles_eng.copy()\n",
    "df['author'] = df['author'].map(lambda x: x.split(',') if str(x) != \"nan\" else \"nan\")\n",
    "df['main topic'] = df['main topic'].map(lambda x: x.split(',') if str(x) != \"nan\" else \"nan\")\n",
    "\n",
    "#df[\"author\"] = df[\"author\"].apply(lambda x: i.lower().replace(' ','') for i in x)\n",
    "l = []\n",
    "for index, row in df.iterrows():\n",
    "    l.append([x.lower().replace(' ','') for x in row['author']])\n",
    "df[\"author\"] = l\n",
    "#    row['main topic'] = [x.lower().replace(' ','') for x in row['main topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "columns = ['main topic', 'author', 'title_processed']\n",
    "for index, row in df.iterrows():\n",
    "    words = ''\n",
    "    for col in columns:\n",
    "        if str(row[col]) != \"nan\":\n",
    "            words += ' '.join(row[col]) + ' '\n",
    "    l.append(words)\n",
    "df[\"Bag_of_words\"] = l    \n",
    "df = df[['title','Bag_of_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Bag_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Princess Poppy: The Big Mix Up</td>\n",
       "      <td>YFB janeylouisejones princess poppi big mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Queen 1</td>\n",
       "      <td>YFH victoriaaveyard red queen 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Wir spielen Kasperltheater</td>\n",
       "      <td>ATDH petermitschitczek wir spielen kasperltheat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Harry Potter 4 and the Goblet of Fire</td>\n",
       "      <td>YFH joannek.rowling harri potter 4 goblet fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>A Vow So Bold and Deadly</td>\n",
       "      <td>YFHR brigidkemmerer vow bold deadli</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title  \\\n",
       "0          Princess Poppy: The Big Mix Up   \n",
       "2                             Red Queen 1   \n",
       "21             Wir spielen Kasperltheater   \n",
       "23  Harry Potter 4 and the Goblet of Fire   \n",
       "25               A Vow So Bold and Deadly   \n",
       "\n",
       "                                        Bag_of_words  \n",
       "0       YFB janeylouisejones princess poppi big mix   \n",
       "2                   YFH victoriaaveyard red queen 1   \n",
       "21  ATDH petermitschitczek wir spielen kasperltheat   \n",
       "23   YFH joannek.rowling harri potter 4 goblet fire   \n",
       "25              YFHR brigidkemmerer vow bold deadli   "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "count_matrix = count.fit_transform(df['Bag_of_words'])\n",
    "cosine_sim = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "b5f4qIPUm28s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cosine_similarity_matrix(df, lang):\n",
    "#    #Converting the book title into vectors and used bigram\n",
    "#    tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df = 1, stop_words=lang)\n",
    "#    tfidf_matrix = tf.fit_transform(df['title'])\n",
    "#    cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine_similarity_matrix(titles_eng, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(title, cosine_sim = cosine_similarities):\n",
    "    # initializing the empty list of recommended movies\n",
    "    recommendations = [] \n",
    "    indices = pd.Series(df['title'])\n",
    "    \n",
    "    # gettin the index of the book that matches the title\n",
    "    idx = indices[indices == title].index[0]\n",
    "    \n",
    "    ##if title not in idx:\n",
    "    ##    raise KeyError(\"title is not in indices\")\n",
    "    \n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
    "\n",
    "    # getting the indices of the 5 most similar books\n",
    "    top_5_indices = list(score_series.iloc[1:6].index)\n",
    "\n",
    "    # populating the list with the titles of the best 5 matching books\n",
    "    for i in top_5_indices:\n",
    "        recommendations.append(list(df[\"title\"])[i])\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Doughnut Street School And The Mystery Of The Doughnut Hole',\n",
       " \"The Inventor's Clone\",\n",
       " 'The Roman, the Twelve & the King',\n",
       " 'The 13th Apostle',\n",
       " 'A Sword Named Truth']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations(\"Anne of Green Gables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "AUPp17pKmtRf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Secret Garden                                                                                       25\n",
       "Anne of Green Gables                                                                                    19\n",
       "Alice's Adventures in Wonderland                                                                        18\n",
       "The Railway Children                                                                                    17\n",
       "A Christmas Carol                                                                                       14\n",
       "                                                                                                        ..\n",
       "Fancy Nancy: Nancy Clancy, Super Sleuth                                                                  1\n",
       "Chroniken von Chaos und Ordnung Band 1-4: Thorn Gandir / Telos Malakin / Bargh Barrowson / Lucretia      1\n",
       "Shard Abyss                                                                                              1\n",
       "The Secret Message                                                                                       1\n",
       "The Heart of a King                                                                                      1\n",
       "Name: title, Length: 37373, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_eng.title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def recommend(title):# Convert the index into series\n",
    "    indices = pd.Series(titles_eng.index, index = titles_eng['title'])\n",
    "    \n",
    "    #Converting the book title into vectors and used bigram\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(2, 2), min_df = 1, stop_words='english')\n",
    "    tfidf_matrix = tf.fit_transform(titles_eng['title'])\n",
    "    \n",
    "    # Calculating the similarity measures based on Cosine Similarity\n",
    "    sg = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Get the index corresponding to original_title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores \n",
    "    sig = list(enumerate(sg[idx]))\n",
    "    # Sort the books\n",
    "    sig = sorted(sig, key=lambda x: x[1], reverse=True)\n",
    "    # Scores of the 5 most similar books \n",
    "    sig = sig[1:6]\n",
    "    # Book indicies\n",
    "    book_indices = [i[0] for i in sig]\n",
    "   \n",
    "    # Top 5 book recommendation\n",
    "    rec = titles_eng[['title']].iloc[book_indices]\n",
    "    print(rec)'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Title_analyzing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
